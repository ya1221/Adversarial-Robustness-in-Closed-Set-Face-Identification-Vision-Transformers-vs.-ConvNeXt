{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":34595,"sourceType":"datasetVersion","datasetId":26922},{"sourceId":7467346,"sourceType":"datasetVersion","datasetId":4346662}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-25T00:26:43.384990Z\",\"iopub.execute_input\":\"2026-01-25T00:26:43.385786Z\",\"iopub.status.idle\":\"2026-01-25T00:26:43.388945Z\",\"shell.execute_reply.started\":\"2026-01-25T00:26:43.385754Z\",\"shell.execute_reply\":\"2026-01-25T00:26:43.388167Z\"}}\n!pip -q install facenet-pytorch\nimport sys, subprocess\n!pip -q install timm facenet-pytorch\n\n# %% [code]\nimport os\nimport torch\nimport torch.nn.functional as F\nimport timm\nfrom PIL import Image\nfrom facenet_pytorch import MTCNN\nimport glob\n!pip -q install tqdm\nfrom tqdm.auto import tqdm\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\nimport cv2\n\n!pip uninstall -y mxnet-cu112 mxnet-cu110 mxnet\n!pip install mxnet\n!pip -q install torchattacks timm\n\nnp.bool = np.bool_ \n\nimport mxnet as mx \nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport io\nimport copy\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-25T00:26:43.410446Z\",\"iopub.execute_input\":\"2026-01-25T00:26:43.410659Z\",\"iopub.status.idle\":\"2026-01-25T00:26:44.214062Z\",\"shell.execute_reply.started\":\"2026-01-25T00:26:43.410639Z\",\"shell.execute_reply\":\"2026-01-25T00:26:44.213233Z\"}}\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmtcnn = MTCNN(image_size=112, margin=0, post_process=True, device=device)\n\ncnn = timm.create_model(\n    \"hf_hub:gaunernst/convnext_nano.cosface_ms1mv3\",\n    pretrained=True\n).to(device).eval()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-25T00:26:44.215054Z\",\"iopub.execute_input\":\"2026-01-25T00:26:44.215332Z\",\"iopub.status.idle\":\"2026-01-25T00:26:53.844777Z\",\"shell.execute_reply.started\":\"2026-01-25T00:26:44.215308Z\",\"shell.execute_reply\":\"2026-01-25T00:26:53.843973Z\"}}\n\nprint(\"Datasets under /kaggle/input:\")\nprint(os.listdir(\"/kaggle/input\")[:50])\n\nimgs = glob.glob(\"/kaggle/input/**/*.*\", recursive=True)\nimgs = [p for p in imgs if p.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"))]\nprint(\"num images found:\", len(imgs))\nprint(\"first 20 images:\")\nprint(\"\\n\".join(imgs[:20]))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-25T00:26:57.053706Z\",\"iopub.execute_input\":\"2026-01-25T00:26:57.053949Z\",\"iopub.status.idle\":\"2026-01-25T00:30:18.463158Z\",\"shell.execute_reply.started\":\"2026-01-25T00:26:57.053920Z\",\"shell.execute_reply\":\"2026-01-25T00:30:18.462336Z\"}}\n\n\n\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"  \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nBATCH_SIZE = 64\nMAX_IDENTITIES = 2000        \nMAX_PROBES_PER_ID = 3        \nGALLERY_PER_ID = 1           \n\ndef find_dataset_root():\n    candidates = []\n    for root in glob.glob(\"/kaggle/input/**\", recursive=True):\n        if not os.path.isdir(root): \n            continue\n        subdirs = [d for d in glob.glob(os.path.join(root, \"*\")) if os.path.isdir(d)]\n        if len(subdirs) == 0:\n            continue\n        img_count = 0\n        for sd in subdirs[:10]:\n            img_count += len(glob.glob(os.path.join(sd, \"*.jpg\")))\n            img_count += len(glob.glob(os.path.join(sd, \"*.png\")))\n            if img_count > 20:\n                candidates.append(root)\n                break\n    if not candidates:\n        raise RuntimeError(\"No dataset folders found under /kaggle/input. Did you add a dataset via 'Add data'?\")\n    candidates = sorted(set(candidates), key=lambda p: -p.count(os.sep))\n    return candidates[0]\n\nDATA_ROOT = find_dataset_root()\nprint(\"Using DATA_ROOT:\", DATA_ROOT)\n\n\n\ndef build_id_map(root):\n    id2paths = defaultdict(list)\n    for person_dir in glob.glob(os.path.join(root, \"*\")):\n        if not os.path.isdir(person_dir):\n            continue\n        person = os.path.basename(person_dir)\n        imgs = []\n        for ext in (\"jpg\", \"jpeg\", \"png\", \"bmp\", \"webp\"):\n            imgs += glob.glob(os.path.join(person_dir, f\"*.{ext}\"))\n        if len(imgs) >= 2:  \n            id2paths[person].extend(sorted(imgs))\n    return id2paths\n\nid2paths = build_id_map(DATA_ROOT)\nprint(\"Identities with >=2 images:\", len(id2paths))\n\nids = sorted(list(id2paths.keys()))\nrandom.shuffle(ids)\nids = ids[:min(MAX_IDENTITIES, len(ids))]\n\ngallery_paths = []\ngallery_labels = []\nprobe_paths = []\nprobe_labels = []\n\nlabel_map = {name: i for i, name in enumerate(ids)}\n\nfor name in ids:\n    paths = id2paths[name]\n    random.shuffle(paths)\n    gallery_paths.append(paths[0])\n    gallery_labels.append(label_map[name])\n\n    probes = paths[1:1+MAX_PROBES_PER_ID]\n    for p in probes:\n        probe_paths.append(p)\n        probe_labels.append(label_map[name])\n\nprint(\"Gallery:\", len(gallery_paths), \"Probes:\", len(probe_paths))\n\nmtcnn = MTCNN(image_size=112, margin=0, post_process=True, device=device)\n\n\n\nREFERENCE_PTS = np.array([\n    [38.2946, 51.6963],  # left eye\n    [73.5318, 51.6963],  # right eye\n    [56.0252, 71.7366],  # nose\n    [41.5493, 92.3655],  # left mouth corner\n    [70.7299, 92.3655]   # right mouth corner\n], dtype=np.float32)\n\ndef align_face_warp(img, landmarks):\n    if isinstance(img, Image.Image):\n        img = np.array(img)\n        \n    src_pts = np.array(landmarks, dtype=np.float32)\n    \n    M, _ = cv2.estimateAffinePartial2D(src_pts, REFERENCE_PTS)\n    \n    warped = cv2.warpAffine(img, M, (112, 112))\n    \n    return Image.fromarray(warped)\n\ndef align_one(path):\n    img = Image.open(path).convert(\"RGB\")\n    \n    boxes, probs, points = mtcnn.detect(img, landmarks=True)\n    \n    if boxes is None:\n        return None\n        \n    best_idx = np.argmax(probs)\n    landmarks = points[best_idx] \n    \n    face_aligned = align_face_warp(img, landmarks)\n    \n    face_tensor = torch.tensor(np.array(face_aligned)).permute(2,0,1).float()\n    face_tensor = (face_tensor - 127.5) / 128.0 \n    \n    return face_tensor\n\ndef align_paths(paths, desc=\"align\"):\n    faces = []\n    kept_paths = []\n    for p in tqdm(paths, desc=desc, unit=\"img\"):\n        face = align_one(p)\n        if face is not None:\n            faces.append(face)\n            kept_paths.append(p)\n    return faces, kept_paths\n\n\ngallery_faces, gallery_paths_kept = align_paths(gallery_paths, \"align gallery\")\nprobe_faces, probe_paths_kept = align_paths(probe_paths, \"align probe\")\n\ngallery_labels_kept = [gallery_labels[gallery_paths.index(p)] for p in gallery_paths_kept]\nprobe_labels_kept   = [probe_labels[probe_paths.index(p)] for p in probe_paths_kept]\n\nprint(\"Kept gallery:\", len(gallery_faces), \"Kept probes:\", len(probe_faces))\n\ncnn = timm.create_model(\"hf_hub:gaunernst/convnext_nano.cosface_ms1mv3\", pretrained=True).to(device).eval()\n\n@torch.no_grad()\ndef embed(model, faces, batch_size=BATCH_SIZE, desc=\"embed\"):\n    embs = []\n    for i in tqdm(range(0, len(faces), batch_size), desc=desc, unit=\"batch\"):\n        x = torch.stack(faces[i:i+batch_size]).to(device)  \n        y = model(x)\n        y = F.normalize(y, dim=1)\n        embs.append(y.detach().cpu())\n    return torch.cat(embs, dim=0)\n\ng_cnn = embed(cnn, gallery_faces)\np_cnn = embed(cnn, probe_faces)\n\n\ndef identification_metrics(g_emb, g_labels, p_emb, p_labels, max_rank=20):\n    sim = p_emb @ g_emb.T \n    g_labels_t = torch.tensor(g_labels)\n    p_labels_t = torch.tensor(p_labels)\n\n    topk = torch.topk(sim, k=min(max_rank, sim.shape[1]), dim=1).indices  \n    topk_labels = g_labels_t[topk]  \n\n    correct = (topk_labels == p_labels_t.unsqueeze(1))  \n    rank1 = correct[:, 0].float().mean().item()\n    rank5 = correct[:, :5].any(dim=1).float().mean().item() if sim.shape[1] >= 5 else float(\"nan\")\n\n    cmc = []\n    for r in range(1, min(max_rank, sim.shape[1]) + 1):\n        cmc.append(correct[:, :r].any(dim=1).float().mean().item())\n    return rank1, rank5, cmc\n\nrank1_cnn, rank5_cnn, cmc_cnn = identification_metrics(g_cnn, gallery_labels_kept, p_cnn, probe_labels_kept)\n\nprint(f\"CNN  - Rank-1: {rank1_cnn:.4f} | Rank-5: {rank5_cnn:.4f}\")\n\nplt.figure()\nplt.plot(range(1, len(cmc_cnn)+1), cmc_cnn, label=\"CNN\")\nplt.xlabel(\"Rank\")\nplt.ylabel(\"Identification Rate (CMC)\")\nplt.title(\"Closed-set Identification on LFW-style split\")\nplt.legend()\nplt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-25T00:30:18.464381Z\",\"iopub.execute_input\":\"2026-01-25T00:30:18.464712Z\",\"iopub.status.idle\":\"2026-01-25T05:59:28.583252Z\",\"shell.execute_reply.started\":\"2026-01-25T00:30:18.464685Z\",\"shell.execute_reply\":\"2026-01-25T05:59:28.582161Z\"}}\n\nCONFIG = {\n    'batch_size': 64,\n    'lr': 1e-4,                 \n    'epochs': 10,               \n    'max_epsilon': 0.1,         \n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'rec_path': '/kaggle/input/ms1mv3/ms1m-retinaface-t1' \n}\n\nclass MXFaceDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(MXFaceDataset, self).__init__()\n        self.transform = transform\n        self.root_dir = root_dir\n        path_imgrec = os.path.join(root_dir, 'train.rec')\n        path_imgidx = os.path.join(root_dir, 'train.idx')\n        self.imgrec = mx.recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')\n        s = self.imgrec.read_idx(0)\n        header, _ = mx.recordio.unpack(s)\n        if header.flag > 0:\n            self.imgidx = np.array(range(1, int(header.label[1])))\n        else:\n            self.imgidx = np.array(list(self.imgrec.keys))\n\n    def __getitem__(self, index):\n        while True:\n            try:\n                idx = self.imgidx[index]\n                s = self.imgrec.read_idx(idx)\n                header, img_bytes = mx.recordio.unpack(s)\n                if len(img_bytes) == 0: raise ValueError\n                img = Image.open(io.BytesIO(img_bytes))\n                if img.mode != 'RGB': img = img.convert('RGB')\n                if self.transform: img = self.transform(img)\n                return img, 0 \n            except:\n                index = (index + 1) % len(self.imgidx)\n\n    def __len__(self):\n        return len(self.imgidx)\n\ntransform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nprint(\"Loading Dataset...\")\ntrain_dataset = MXFaceDataset(root_dir=CONFIG['rec_path'], transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, drop_last=True)\n\nmodel_name = \"hf_hub:gaunernst/convnext_nano.cosface_ms1mv3\"\n\nprint(f\"Initializing Teacher ({model_name})...\")\nteacher_model = timm.create_model(model_name, pretrained=True).to(CONFIG['device'])\nteacher_model.eval()\nfor param in teacher_model.parameters(): param.requires_grad = False\n\nprint(f\"Initializing Student ({model_name})...\")\nstudent_model = timm.create_model(model_name, pretrained=True).to(CONFIG['device'])\nstudent_model.train()\n\noptimizer = optim.Adam(student_model.parameters(), lr=CONFIG['lr'])\n\ndef train_pgd_attack(model, images, target_embs, eps, alpha=2/255, steps=5):\n    adv_images = images.clone().detach()\n    for _ in range(steps):\n        adv_images.requires_grad = True\n        curr_emb = torch.nn.functional.normalize(model(adv_images), dim=1)\n        loss = (curr_emb * target_embs).sum()\n        model.zero_grad()\n        loss.backward()\n        adv_images = adv_images - alpha * adv_images.grad.sign()\n        eta = torch.clamp(adv_images - images, min=-eps, max=eps)\n        adv_images = torch.clamp(images + eta, min=-1, max=1).detach()\n    return adv_images\n\nprint(\"\\n=== Starting CNN Curriculum Training (FGSM -> PGD) ===\")\nMAX_BATCHES = 2000 \n\nfor epoch in range(CONFIG['epochs']):\n    student_model.train()\n    running_loss = 0.0\n    \n    current_eps = (epoch / CONFIG['epochs']) * CONFIG['max_epsilon']\n    if epoch == 0: current_eps = 0.01 \n    \n    attack_type = 'FGSM'\n    if epoch >= CONFIG['epochs'] // 2: \n        attack_type = 'PGD'\n    \n    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Eps: {current_eps:.4f} | Mode: {attack_type}\")\n    \n    loop = tqdm(enumerate(train_loader), total=min(len(train_loader), MAX_BATCHES), leave=True)\n    \n    for i, (images, _) in loop: \n        if i >= MAX_BATCHES: break\n        images = images.to(CONFIG['device'])\n        \n        with torch.no_grad():\n            teacher_emb = torch.nn.functional.normalize(teacher_model(images), dim=1)\n        \n        if current_eps > 0:\n            if attack_type == 'FGSM':\n                delta = torch.zeros_like(images, requires_grad=True)\n                adv_emb_tmp = torch.nn.functional.normalize(student_model(images + delta), dim=1)\n                loss_attack = -(adv_emb_tmp * teacher_emb).sum()\n                loss_attack.backward()\n                adv_images = images + current_eps * delta.grad.detach().sign()\n                adv_images = torch.clamp(adv_images, -1, 1)\n                \n            elif attack_type == 'PGD':\n                adv_images = train_pgd_attack(student_model, images, teacher_emb, current_eps, steps=5)\n        else:\n            adv_images = images\n\n        optimizer.zero_grad()\n        student_adv_emb = torch.nn.functional.normalize(student_model(adv_images), dim=1)\n        student_clean_emb = torch.nn.functional.normalize(student_model(images), dim=1)\n        loss_robust = 1 - (student_adv_emb * teacher_emb).sum(dim=1).mean()\n        loss_clean = 1 - (student_clean_emb * teacher_emb).sum(dim=1).mean()\n        loss = 0.5 * loss_robust + 0.5 * loss_clean\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        loop.set_postfix(loss=loss.item(), mode=attack_type)\n\n    print(f\"Epoch Finished. Avg Loss: {running_loss/(i+1):.4f}\")\n    torch.save(student_model.state_dict(), \"convnext_curriculum_trained.pth\")\n\nprint(\"Done! CNN Model saved.\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-25T05:59:28.584915Z\",\"iopub.execute_input\":\"2026-01-25T05:59:28.585235Z\",\"iopub.status.idle\":\"2026-01-25T06:04:27.342556Z\",\"shell.execute_reply.started\":\"2026-01-25T05:59:28.585202Z\",\"shell.execute_reply\":\"2026-01-25T06:04:27.341840Z\"}}\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 32\nEPSILONS = [0.0, 0.01, 0.03, 0.05, 0.1]\nSQUARE_ITER = 20 \n\nprint(f\"Running CNN Evaluation on: {device}\")\n\nmodel_name = \"hf_hub:gaunernst/convnext_nano.cosface_ms1mv3\"\nprint(f\"Loading Model: {model_name}\")\nmodel = timm.create_model(model_name, pretrained=False).to(device)\n\ncheckpoint_path = \"convnext_curriculum_trained.pth\"\n\nif os.path.exists(checkpoint_path):\n    model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n    print(f\"Weights Loaded from {checkpoint_path}\")\nelse:\n    print(\"WARNING: Model weights file not found! Using random weights.\")\n\nmodel.eval()\n\nvalid_gallery_ids = set(gallery_labels_kept)\nfiltered_probes = []\nfiltered_labels = []\n\nfor face, lbl in zip(probe_faces, probe_labels_kept):\n    if lbl in valid_gallery_ids:\n        filtered_probes.append(face)\n        filtered_labels.append(lbl)\n\nprobe_faces_tensor = torch.stack(filtered_probes)\nprint(f\"Valid Test Set: {len(probe_faces_tensor)} images\")\n\nprint(\"Computing Gallery Embeddings (CNN)...\")\ngallery_loader = torch.utils.data.DataLoader(torch.stack(gallery_faces), batch_size=64)\ngallery_embs = []\nwith torch.no_grad():\n    for batch in gallery_loader:\n        batch = batch.to(device)\n        gallery_embs.append(F.normalize(model(batch), dim=1))\ngallery_embs = torch.cat(gallery_embs)\n\n\ndef get_metrics(probe_emb, probe_lbls, gallery_emb, gallery_lbls):\n    sim_matrix = probe_emb @ gallery_emb.T\n    max_scores, max_indices = torch.max(sim_matrix, dim=1)\n    correct = 0\n    total = len(probe_lbls)\n    sim_matrix = sim_matrix.cpu()\n    for i in range(total):\n        if gallery_lbls[max_indices[i].item()] == probe_lbls[i]:\n            correct += 1\n    return correct, total\n\ndef manual_square_attack(model, images, target_embs, eps, n_queries=20):\n    if eps == 0: return images\n    adv_images = images.clone().detach()\n    \n    with torch.no_grad():\n        curr_emb = F.normalize(model(adv_images), dim=1)\n        best_loss = (curr_emb * target_embs).sum(dim=1) \n    \n    for _ in range(n_queries):\n        noise = torch.randn_like(images).sign() * eps\n        candidate = torch.clamp(adv_images + noise, -1, 1)\n        \n        with torch.no_grad():\n            cand_emb = F.normalize(model(candidate), dim=1)\n            cand_loss = (cand_emb * target_embs).sum(dim=1)\n            is_better = cand_loss < best_loss\n            adv_images[is_better] = candidate[is_better]\n            best_loss[is_better] = cand_loss[is_better]\n            \n    return adv_images\n\n\nresults = []\nATTACKS = ['FGSM', 'Square']\n\nprint(\"\\n=== Starting CNN Evaluation ===\")\n\nfor eps in EPSILONS:\n    print(f\"\\nProcessing Epsilon: {eps}\")\n    \n    num_batches = (len(probe_faces_tensor) + BATCH_SIZE - 1) // BATCH_SIZE\n    attack_stats = {atk: {'correct': 0, 'drift': 0.0} for atk in ATTACKS}\n    attack_stats['Clean'] = {'correct': 0, 'drift': 0.0}\n    \n    batch_iterator = tqdm(range(num_batches), desc=f\"Eps {eps}\", leave=False)\n    \n    for i in batch_iterator:\n        start = i * BATCH_SIZE\n        end = min(start + BATCH_SIZE, len(probe_faces_tensor))\n        \n        batch_imgs = probe_faces_tensor[start:end].to(device)\n        batch_lbls = filtered_labels[start:end]\n        \n        with torch.no_grad():\n            clean_emb = F.normalize(model(batch_imgs), dim=1)\n        \n        c, t = get_metrics(clean_emb, batch_lbls, gallery_embs, gallery_labels_kept)\n        attack_stats['Clean']['correct'] += c\n        \n        if eps == 0: continue\n\n        for attack_name in ATTACKS:\n            if attack_name == 'FGSM':\n                img_grad = batch_imgs.clone().detach()\n                img_grad.requires_grad = True\n                curr = F.normalize(model(img_grad), dim=1)\n                loss = (curr * clean_emb).sum() \n                model.zero_grad()\n                loss.backward()\n                adv_imgs = batch_imgs - eps * img_grad.grad.sign()\n                adv_imgs = torch.clamp(adv_imgs, -1, 1)\n\n            elif attack_name == 'Square':\n                adv_imgs = manual_square_attack(model, batch_imgs, clean_emb, eps, n_queries=SQUARE_ITER)\n\n            with torch.no_grad():\n                adv_emb = F.normalize(model(adv_imgs), dim=1)\n            \n            acc_c, _ = get_metrics(adv_emb, batch_lbls, gallery_embs, gallery_labels_kept)\n            attack_stats[attack_name]['correct'] += acc_c\n            \n            batch_drift = torch.norm(clean_emb - adv_emb, dim=1).sum().item()\n            attack_stats[attack_name]['drift'] += batch_drift\n\n    total_samples = len(probe_faces_tensor)\n    clean_acc = attack_stats['Clean']['correct'] / total_samples\n    \n    if eps == 0:\n        for atk in ATTACKS:\n             results.append({\"Attack\": atk, \"Epsilon\": 0.0, \"Accuracy\": clean_acc, \"ASR\": 0.0, \"Drift\": 0.0})\n    else:\n        for atk in ATTACKS:\n            acc = attack_stats[atk]['correct'] / total_samples\n            drift = attack_stats[atk]['drift'] / total_samples\n            asr = max(0, clean_acc - acc)\n            results.append({\"Attack\": atk, \"Epsilon\": eps, \"Accuracy\": acc, \"ASR\": asr, \"Drift\": drift})\n\ndf = pd.DataFrame(results)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"     CNN FINAL RESULTS\")\nprint(\"=\"*50)\nprint(df.round(4))\nprint(\"=\"*50)\n\ndf.to_csv(\"cnn_robustness_results.csv\", index=False)\nprint(\"Saved to cnn_robustness_results.csv\")\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 6))\nplt.style.use('seaborn-v0_8-whitegrid')\n\nsns.lineplot(data=df, x=\"Epsilon\", y=\"Accuracy\", hue=\"Attack\", style=\"Attack\", markers=True, ax=axes[0], linewidth=3, markersize=10)\naxes[0].set_title(\"CNN Robustness: Accuracy\", fontsize=15, fontweight='bold')\naxes[0].set_ylabel(\"Accuracy\")\n\nsns.lineplot(data=df, x=\"Epsilon\", y=\"ASR\", hue=\"Attack\", style=\"Attack\", markers=True, ax=axes[1], linewidth=3, markersize=10)\naxes[1].set_title(\"CNN Vulnerability: ASR\", fontsize=15, fontweight='bold')\naxes[1].set_ylabel(\"Accuracy Drop\")\n\nsns.lineplot(data=df, x=\"Epsilon\", y=\"Drift\", hue=\"Attack\", style=\"Attack\", markers=True, ax=axes[2], linewidth=3, markersize=10)\naxes[2].set_title(\"CNN Stability: Drift\", fontsize=15, fontweight='bold')\naxes[2].set_ylabel(\"L2 Distance\")\n\nplt.tight_layout()\nplt.savefig('cnn_final_graphs.png')\nplt.show()","metadata":{"_uuid":"ecf6817e-60ed-46af-8b47-9d9d33e012c0","_cell_guid":"2d2aa4bd-56bd-421c-9d16-237edf4f9ff2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}